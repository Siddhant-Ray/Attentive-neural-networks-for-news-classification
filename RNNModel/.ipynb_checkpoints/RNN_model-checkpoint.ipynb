{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "invalid-postcard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/sidray/Attentive-recurrent-neural-networks-for-categorizing-and-generating-news/news_test_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from io import open \n",
    "\n",
    "\"\"\"Writing the data per file into a dictionary for which the key is the category of news\"\"\"\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "all_letters = string.ascii_letters + \".,;'& \"\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "def unitoAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "if os.path.isdir(cwd):\n",
    "    os.chdir(\"..\")\n",
    "    \n",
    "path = os.getcwd()\n",
    "path_news = os.path.join(path, \"news_test_data\")\n",
    "print(path_news)\n",
    "\n",
    "def readFile_byline(filename):\n",
    "    line_of_news = open(path_news+\"/\"+filename , encoding = 'utf-8').read().split('\\n')    \n",
    "    return [unitoAscii(line) for line in line_of_news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polish-occasions",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/sidray/Attentive-recurrent-neural-networks-for-categorizing-and-generating-news/news_test_dataBUSINESS.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c23e2e13b341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcategory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mall_categories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnews_descp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadFile_byline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mcategory_news\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews_descp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-727144c149d5>\u001b[0m in \u001b[0;36mreadFile_byline\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadFile_byline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mline_of_news\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_news\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0munitoAscii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_of_news\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch/sidray/Attentive-recurrent-neural-networks-for-categorizing-and-generating-news/news_test_dataBUSINESS.txt'"
     ]
    }
   ],
   "source": [
    "category_news = {}\n",
    "all_categories = []\n",
    "\n",
    "list_of_files = os.listdir(\"news_data_test/\")\n",
    "\n",
    "#Dictionary which maps every category of news to it's description\n",
    "for _file in list_of_files:\n",
    "    category = _file.split(\".\")[0]\n",
    "    all_categories.append(category)\n",
    "    news_descp = readFile_byline(_file)\n",
    "    category_news[category] = news_descp\n",
    "\n",
    "num_categories = len(all_categories)\n",
    "print(num_categories)\n",
    "\n",
    "count_of_news_category = {}\n",
    "average_words_per_number_of_samples = []\n",
    "\n",
    "#Counting news items per category of news \n",
    "for key in category_news.keys():\n",
    "    item = category_news.get(key)\n",
    "    number_of_news_items = len(item)\n",
    "    count_of_news_category[key] = number_of_news_items\n",
    "    count_of_words = 0\n",
    "    for sentence in item:\n",
    "        count_of_words += len(sentence.split(\" \"))\n",
    "    average_words_per_number_of_samples.append(count_of_words/number_of_news_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "fig = plt.figure()\n",
    "category = count_of_news_category.keys()\n",
    "value = count_of_news_category.values()\n",
    "plt.bar(category, value)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('Number_of_articles_per_category.png')\n",
    "plt.show()\n",
    "\n",
    "fig1 = plt.figure()\n",
    "category = count_of_news_category.keys()\n",
    "value = average_words_per_number_of_samples\n",
    "plt.bar(category, value)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('Words_per_category.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch import optim\n",
    "import torch.nn.functional as F \n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n",
    "\n",
    "category_plus_news_list = []\n",
    "\n",
    "for key in category_news.keys():\n",
    "    for news_item in category_news[key]:\n",
    "        category_plus_news_list.append((key, news_item))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "from InferSentModel import InferSent\n",
    "model_version = 2\n",
    "MODEL_PATH = \"encoder/infersent%s.pkl\" % model_version\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': model_version}\n",
    "model = InferSent(params_model)\n",
    "model.load_state_dict(torch.load(MODEL_PATH))\n",
    "\n",
    "\n",
    "use_cuda = True\n",
    "model = model.to(torch.device('cuda:0')) if use_cuda else model\n",
    "\n",
    "W2V_PATH = 'fastText/crawl-300d-2M.vec'\n",
    "model.set_w2v_path(W2V_PATH)\n",
    "model.build_vocab_k_words(K=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences = []\n",
    "\n",
    "labels =[]\n",
    "count_of_lables=[]\n",
    "\n",
    "for pair in category_plus_news_list:\n",
    "    label = pair[0]\n",
    "    sentences = pair[1]\n",
    "    \n",
    "    if len(sentences.split(\" \")) >= 5:\n",
    "    \n",
    "        list_of_sentences.append(sentences)\n",
    "        labels.append(label)\n",
    "        \n",
    "#print(list_of_sentences[0:20])\n",
    "print(len(list_of_sentences))\n",
    "#print((labels[0:20]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_business = model.encode(list_of_sentences_business, bsize=128, tokenize=False, verbose=True)\n",
    "embeddings = model.encode(list_of_sentences, bsize=128, tokenize=False, verbose=True)\n",
    "#embeddings_politics = model.encode(list_of_sentences_politics, bsize=128, tokenize=False, verbose=True)\n",
    "#embeddings_religion = model.encode(list_of_sentences_religion, bsize=128, tokenize=False, verbose=True)\n",
    "#embeddings_food = model.encode(list_of_sentences_food, bsize=128, tokenize=False, verbose=True)\n",
    "#embeddings_home = model.encode(list_of_sentences_home, bsize=128, tokenize=False, verbose=True)\n",
    "#print('nb sentences encoded : {0}'.format(len(embeddings_business)))\n",
    "#print(embeddings_business.shape)\n",
    "print('nb sentences encoded : {0}'.format(len(embeddings)))\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(category_news.keys())\n",
    "index_class_map_dict={}\n",
    "\n",
    "for idx, value in enumerate(label_list):\n",
    "    index_class_map_dict[value]=idx\n",
    "\n",
    "print(index_class_map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olympic-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sentences = []\n",
    "\n",
    "for index, embedding in enumerate(embeddings):\n",
    "    input_vector = embedding\n",
    "    target_vector = labels[index]\n",
    "    target_class = index_class_map_dict[target_vector]\n",
    "    embedded_sentences.append((input_vector, target_class))\n",
    "\n",
    "    \n",
    "#print(embedded_sentences[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''label_list = list(category_news.keys())\n",
    "\n",
    "for label in labels:\n",
    "    for idx, value in enumerate(label_list):\n",
    "        if label == value:\n",
    "            labels[labels.index(label)] = idx'''\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-export",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(category_news.keys())\n",
    "index_class_map=[]\n",
    "\n",
    "for idx, value in enumerate(label_list):\n",
    "    index_class_map.append((idx,value))\n",
    "\n",
    "print(index_class_map)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(category_news.keys())\n",
    "index_class_map=[]\n",
    "\n",
    "for idx, value in enumerate(label_list):\n",
    "    index_class_map.append((idx,value))\n",
    "\n",
    "print(index_class_map)\n",
    "\n",
    "\n",
    "label_list = list(category_news.keys())\n",
    "index_class_map_dict={}\n",
    "\n",
    "for idx, value in enumerate(label_list):\n",
    "    index_class_map_dict[idx]=value\n",
    "\n",
    "print(index_class_map_dict.keys())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-inventory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "class SNNLinear(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        nn.init.normal_(self.fc.weight, std = math.sqrt(1/input_size))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.fc(inputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-albuquerque",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "class NN(nn.Module):\n",
    "    \"\"\" Simple NN architecture with 3 fully connected layers\n",
    "        and SeLU activation \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        fc1 = SNNLinear(input_size, hidden_size)\n",
    "        fc2 = SNNLinear(hidden_size, hidden_size//2)\n",
    "        fc3 = SNNLinear(hidden_size//2, output_size)\n",
    "        self.net = nn.Sequential(fc1, nn.SELU(), nn.AlphaDropout(0.2), fc2, nn.SELU(), nn.AlphaDropout(0.2), fc3) \n",
    "                                #nn.SELU(), nn.AlphaDropout(0.5), fc4)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-radiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_tensor, target_tensor, model, model_optimizer, criterion):\n",
    "    model.train()\n",
    "    \n",
    "    model_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.shape\n",
    "    target_length = target_tensor.shape\n",
    "    \n",
    "    output = model(input_tensor)\n",
    "        \n",
    "    loss = criterion(output, target_tensor)\n",
    "    loss.backward()\n",
    "    \n",
    "    model_optimizer.step()\n",
    "    \n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval(input_tensor, target_tensor, model, model_optimizer, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        input_length = input_tensor.shape\n",
    "        target_length = target_tensor.shape\n",
    "        output = model(input_tensor)\n",
    "\n",
    "        loss = criterion(output, target_tensor)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-exclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_items_per_class = [5827, 15920, 3361, 2438, 6195, 6137, 2730, 2177, 1699, 6524, 1123, 986, 9826, 3941, 8664, 1362, 17768, 4693, 2229, 9601, 1121, 4172, 2109, 2611, 6076, 3421, 5008, 1321, 2078, 32241, 2533, 3459, 3404, 1376, 2067, 4463, 3641, 3821]\n",
    "\n",
    "weights = []\n",
    "for i in num_of_items_per_class:\n",
    "    weights.append(1/i)\n",
    "#print(len(weights))\n",
    "\n",
    "class_weights = torch.FloatTensor(weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removed-stable",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_of_data):\n",
    "        self.list_of_data = list_of_data\n",
    "\n",
    "    # get one sample\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = self.list_of_data[idx]\n",
    "        input_tensor = torch.from_numpy(sample[0]).float()\n",
    "        target_tensor = torch.tensor(sample[1])\n",
    "    \n",
    "        return input_tensor, target_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_of_data)\n",
    "    \n",
    "dataset = Dataset(embedded_sentences)\n",
    "\n",
    "_input, _target = dataset.__getitem__(0)\n",
    "print(_input.shape, _target.shape)\n",
    "\n",
    "\n",
    "val_size = 0.1\n",
    "test_size = 0.1\n",
    "\n",
    "test_amount, val_amount = int(dataset.__len__() * test_size), int(dataset.__len__() * val_size)\n",
    "print(test_amount, val_amount)\n",
    "\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [\n",
    "            (dataset.__len__() - (test_amount + val_amount)), \n",
    "            test_amount, \n",
    "            val_amount\n",
    "])\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "            train_set,\n",
    "            batch_size=128,\n",
    "            shuffle=True,\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "            val_set,\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    ")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "            test_set,\n",
    "            batch_size=128,\n",
    "            shuffle=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import *\n",
    "\n",
    "learning_rate = 1e-4\n",
    "def trainIters(model, n_iters, embedded, val_embedded, print_every, learning_rate=learning_rate):\n",
    "    start = time.time()\n",
    "    plot_losses_train = []\n",
    "    plot_losses_val =[]\n",
    "    print_loss_total_train = 0  # Reset every print_every\n",
    "    plot_loss_total_train = 0  # Reset every plot_every\n",
    "    \n",
    "    print_loss_total_val = 0  # Reset every print_every\n",
    "    plot_loss_total_val = 0  # Reset every plot_every\n",
    "    \n",
    "    print_acc_total_train = 0\n",
    "    plot_acc_total_train = 0\n",
    "    plot_acc_train = []\n",
    "    \n",
    "    print_acc_total_val = 0\n",
    "    plot_acc_total_val = 0\n",
    "    plot_acc_val = []\n",
    "    \n",
    "    train_epochs = []\n",
    "    val_epochs = []\n",
    "\n",
    "    #TODO: Try ADAM\n",
    "    model_optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay = 1e-5)\n",
    "    \n",
    "    #TODO: Learning rate scheduler\n",
    "    scheduler = StepLR(model_optimizer, step_size=50, gamma=0.1)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_steps = n_iters*len(embedded)\n",
    "    \n",
    "    for epoch in range(n_iters):\n",
    "                \n",
    "        for local_step, (_input, _target) in enumerate(embedded, 1):\n",
    "\n",
    "            input_tensor = _input.to(device)\n",
    "            #noise = torch.randn_like(input_tensor) * 1e-3 \n",
    "            #input_tensor = input_tensor + noise\n",
    "            target_tensor = _target.to(device)\n",
    "\n",
    "            output, loss = train(input_tensor, target_tensor, model,\n",
    "             model_optimizer, criterion)\n",
    "            \n",
    "            accuracy = (output.argmax(-1) == target_tensor).float().mean()\n",
    "\n",
    "            print_loss_total_train += loss\n",
    "            plot_loss_total_train += loss\n",
    "            print_acc_total_train += accuracy\n",
    "            plot_acc_total_train += accuracy\n",
    "            \n",
    "\n",
    "            global_step = epoch * len(embedded) + local_step\n",
    "\n",
    "            if global_step % print_every == 0:\n",
    "                print_loss_avg_train = print_loss_total_train / print_every\n",
    "                print_loss_total_train = 0\n",
    "                \n",
    "                print('%s (%d %d%%) train_loss = %.4f' % (timeSince(start, global_step / total_steps),\n",
    "                                             global_step, global_step / total_steps * 100, print_loss_avg_train))\n",
    "\n",
    "\n",
    "        plot_loss_avg_train = plot_loss_total_train / len(embedded)\n",
    "        plot_losses_train.append(plot_loss_avg_train)\n",
    "        \n",
    "        plot_avg_acc_train = plot_acc_total_train / len(embedded)\n",
    "        plot_acc_train.append(plot_avg_acc_train)\n",
    "        \n",
    "        plot_loss_total_train = 0\n",
    "        plot_acc_total_train = 0\n",
    "        \n",
    "        train_epochs.append(epoch)\n",
    "\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "\n",
    "\n",
    "            for (_input, _target) in val_embedded:\n",
    "\n",
    "                input_tensor = _input.to(device)\n",
    "                target_tensor = _target.to(device)\n",
    "\n",
    "                output, loss = _eval(input_tensor, target_tensor, model,\n",
    "                             model_optimizer, criterion)\n",
    "\n",
    "                accuracy = (output.argmax(-1) == target_tensor).float().mean()\n",
    "\n",
    "                print_loss_total_val += loss\n",
    "                plot_loss_total_val += loss\n",
    "                print_acc_total_val += accuracy\n",
    "                plot_acc_total_val += accuracy\n",
    "\n",
    "\n",
    "            print_loss_avg_val = print_loss_total_val / len(val_embedded)\n",
    "            print_loss_total_val = 0\n",
    "            \n",
    "            print_avg_acc = print_acc_total_val/ len(val_embedded)\n",
    "            print_acc_total_val = 0\n",
    "           \n",
    "            print('val_loss = %.4f acc = %.4f' % (print_loss_avg_val, print_avg_acc))\n",
    "\n",
    "            plot_loss_avg_val = plot_loss_total_val / len(val_embedded)\n",
    "            plot_avg_acc_val = plot_acc_total_val / len(val_embedded)\n",
    "            \n",
    "            \n",
    "            plot_losses_val.append(plot_loss_avg_val)\n",
    "            plot_acc_val.append(plot_avg_acc_val)\n",
    "            \n",
    "            plot_loss_total_val = 0\n",
    "            plot_acc_total_val = 0\n",
    "            \n",
    "            val_epochs.append(epoch)\n",
    "       \n",
    "        scheduler.step()\n",
    "        \n",
    "\n",
    "    #print(train_epochs)\n",
    "    #print(val_epochs)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.switch_backend('agg')\n",
    "    import matplotlib.ticker as ticker\n",
    "    import numpy as np\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(train_epochs, plot_losses_train, linewidth=5)\n",
    "    plt.plot(val_epochs, plot_losses_val, linewidth=5)\n",
    "    plt.legend(['train loss', 'val loss'], loc = 'upper right')\n",
    "    plt.savefig('loss.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(val_epochs, plot_acc_val, linewidth=5)\n",
    "    plt.savefig('acc.png')\n",
    "    plt.legend(['val_acc'], loc = 'upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 128\n",
    "input_size = embeddings.shape[1]\n",
    "output_size = 38\n",
    "\n",
    "model = NN(input_size, hidden_size, output_size).to(device)\n",
    "learning_rate = 1e-4\n",
    "#Note : may need more epochs range [100-300]\n",
    "epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "trainIters(model, epochs, train_dataloader, val_dataloader, print_every=5000, learning_rate = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-first",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-climb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-clearance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
